{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correspondence\n",
    "\n",
    "### This notebook replicates the dense correspondence experiments in section 6.4 of \"Field Convolutions for Surface CNNs\" (Mitchel et al. 2021).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File reading and progressbar\n",
    "import os\n",
    "import os.path as osp\n",
    "import progressbar\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "\n",
    "# PyTorch Geometric - used for data loading/processing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Field Convolution modules\n",
    "from nn import FCResNetBlock, LiftBlock, ECHOBlock, TangentPerceptron\n",
    "\n",
    "# Transforms\n",
    "from transforms import FCPrecomp, computeLogXPort, SupportGraph, NormalizeArea, NormalizeAxes\n",
    "\n",
    "# Load the remeshed FAUST dataset (Donati et al. 2020)\n",
    "from datasets import FAUSTRM\n",
    "\n",
    "# Clear your cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Band-limit for field convolution filters\n",
    "band_limit = 1\n",
    "\n",
    "# Number of radial samples\n",
    "n_rings = 3\n",
    "\n",
    "# Filter type (see /nn/field_conv.py)\n",
    "ftype = 1\n",
    "\n",
    "# Number of channels in the network\n",
    "nf = 32\n",
    "\n",
    "# Number of ECHO descriptors to compute in the last layer of the network\n",
    "n_des = 12;\n",
    "\n",
    "# Number of descriptor bins per unit radius\n",
    "# Descriptor resolution will be approximately  PI * (n_bins + 0.5) * (n_bins + 0.5)\n",
    "n_bins = 2;\n",
    "\n",
    "# Filter support radius\n",
    "epsilon = 0.0425;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where dataset is stored\n",
    "# The path to the .zip file containing the data should be\n",
    "# /data/FAUSTRM/raw/FAUSTRM.zip\n",
    "path = osp.join('data', 'FAUSTRM')\n",
    "\n",
    "\n",
    "# Pre-processing operations\n",
    "# Compute convolution support edges, meshes are processed at full resolution \n",
    "# Compute logarithm maps + parallel transport\n",
    "pre_transform = T.Compose((\n",
    "    SupportGraph(epsilon=epsilon),\n",
    "    computeLogXPort()\n",
    "))\n",
    "\n",
    "# Apply a random rotation every time a shape is drawn from the dataloader\n",
    "transform = T.Compose((T.Center(),\n",
    "    T.RandomRotate(45, axis=0),\n",
    "    T.RandomRotate(45, axis=1),\n",
    "    T.RandomRotate(45, axis=2))\n",
    ")\n",
    "\n",
    "# Load test and train splits\n",
    "train_dataset = FAUSTRM(path, True, pre_transform=pre_transform, transform=transform)\n",
    "test_dataset = FAUSTRM(path, False, pre_transform=pre_transform, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "n_classes = 4999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNet\n",
    "\n",
    "##### A sucession of FCResNet blocks with an ECHO block as the final layer.  A learnable gradient-like operation is used to lift scalar features to isometry-equivariant tangent vector fields at the beginning of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organizes edge data at run time to expidte convolutions\n",
    "organizeEdges = FCPrecomp(band_limit=band_limit, n_rings=n_rings, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        ## Learned 'gradient', lifting scalar features to tangent vector features\n",
    "        ## at the beginning of the network\n",
    "        \n",
    "        self.lift = LiftBlock(3, 16, n_rings=n_rings, ftype=ftype)\n",
    "        \n",
    "        ## FCNet - eight FCResNet Blocks, followed by an ECHO block \n",
    "        self.resnet1 = FCResNetBlock(16, nf, band_limit=band_limit, \n",
    "                                     n_rings=n_rings, ftype=ftype)\n",
    "                \n",
    "        self.resnet2 = FCResNetBlock(nf, nf, band_limit=band_limit,\n",
    "                                     n_rings=n_rings, ftype=ftype)\n",
    "        \n",
    "        self.resnet3 = FCResNetBlock(nf, nf, band_limit=band_limit,\n",
    "                                     n_rings=n_rings, ftype=ftype)\n",
    "        \n",
    "        self.resnet4 = FCResNetBlock(nf, nf, band_limit=band_limit,\n",
    "                                     n_rings=n_rings, ftype=ftype)\n",
    "\n",
    "        self.resnet5 = FCResNetBlock(nf, nf, band_limit=band_limit,\n",
    "                                     n_rings=n_rings, ftype=ftype)\n",
    "        \n",
    "        self.resnet6 = FCResNetBlock(nf, nf, band_limit=band_limit,\n",
    "                                     n_rings=n_rings, ftype=ftype)\n",
    "        \n",
    "        self.resnet7 = FCResNetBlock(nf, nf, band_limit=band_limit,\n",
    "                                     n_rings=n_rings, ftype=ftype)\n",
    "        \n",
    "        self.resnet8 = FCResNetBlock(nf, 16, band_limit=band_limit,\n",
    "                                     n_rings=n_rings, ftype=ftype, frontload=True)\n",
    "\n",
    "        ## ECHO Block\n",
    "        self.echo = ECHOBlock(16, nf, n_des=n_des, n_bins=n_bins, \n",
    "                             band_limit=band_limit, n_rings=n_rings, ftype=ftype)\n",
    "        \n",
    "        ## 'Meta' residual connections (in addition to those alredy inside the FCResNet blocks)\n",
    "        self.res1 = TangentPerceptron(16, nf)\n",
    "        \n",
    "        self.res2 = TangentPerceptron(nf, nf)\n",
    "        \n",
    "        self.res3 = TangentPerceptron(nf, nf)\n",
    "        \n",
    "        self.res4 = TangentPerceptron(nf, 16)\n",
    "        \n",
    "        \n",
    "        ## Dropout layer\n",
    "        self.D = nn.Dropout(p=0.5)\n",
    "\n",
    "        ## Final linear layer\n",
    "        self.lin1 = nn.Linear(nf, 256)\n",
    "        self.lin2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        ##########################\n",
    "        ### Organize edge data ###\n",
    "        ##########################\n",
    "        supp_edges, supp_sten, ln, wxp = organizeEdges(data)\n",
    "        \n",
    "        attr_lift = (supp_edges, supp_sten[..., band_limit:(band_limit+2)])\n",
    "        attr_conv = (supp_edges, supp_sten)\n",
    "        attr_echo = (supp_edges, supp_sten, ln, wxp)\n",
    "        \n",
    "        \n",
    "        #############################################\n",
    "        ### Lift scalar features to vector fields ###\n",
    "        #############################################\n",
    "        \n",
    "        x = data.pos[data.sample_idx, :]\n",
    "        \n",
    "        x1 = self.lift(x, *attr_lift)\n",
    "\n",
    "        ##########################\n",
    "        ### Field Convolutions ###\n",
    "        ##########################\n",
    "        \n",
    "        x = self.resnet1(x1, *attr_conv) \n",
    "        \n",
    "        x2 = self.resnet2(x, *attr_conv) + self.res1(x1)\n",
    "        \n",
    "        x = self.resnet3(x2, *attr_conv)\n",
    "        \n",
    "        x3 = self.resnet4(x, *attr_conv) + self.res2(x2)\n",
    "        \n",
    "        x = self.resnet5(x3, *attr_conv)\n",
    "        \n",
    "        x4 = self.resnet6(x, *attr_conv) + self.res3(x3)\n",
    "        \n",
    "        x = self.resnet7(x4, *attr_conv)\n",
    "        \n",
    "        x = self.resnet8(x, *attr_conv) + self.res4(x4)\n",
    "        \n",
    "        #########################################################\n",
    "        ### Compute ECHO descriptors and feed to dense layers ###\n",
    "        #########################################################\n",
    "        \n",
    "        x = self.echo(x, *attr_echo)\n",
    "                    \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.D(x)\n",
    "        x = self.lin2(x)\n",
    "             \n",
    "        return x;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the GPU\n",
    "device = torch.device('cuda')\n",
    "model = Net().to(device)\n",
    "\n",
    "# ADAM Optimizer, lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "## Cross Entropy Loss\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training function\n",
    "## Optional batch_step parameter for gradient accumulation (not used in the paper)\n",
    "def train(batch_step=1):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # Sort out progress bar, displays average loss over last 50 samples\n",
    "    wW = 50;\n",
    "    window = torch.FloatTensor(wW).fill_(0)\n",
    "    \n",
    "    n_data = train_loader.__len__()\n",
    "    widgets = [progressbar.Percentage(), progressbar.Bar(), \n",
    "              progressbar.AdaptiveETA(), ' | ', progressbar.Variable('Loss'),]\n",
    "\n",
    "    bar = progressbar.ProgressBar(max_value=n_data, widgets=widgets)\n",
    "\n",
    "    \n",
    "    ## Zero-out\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    i = 0;\n",
    "    for data in train_loader:\n",
    "            \n",
    "        pred = model(data.to(device))\n",
    "\n",
    "        L = loss(pred, data.y.to(device).squeeze())\n",
    "\n",
    "        if (i < wW):\n",
    "            window[i] = L.item()\n",
    "            wAvg = torch.mean(window[:i])\n",
    "        else:\n",
    "            window = torch.cat((window[1:], torch.tensor([L.item()])), dim=0)\n",
    "            wAvg = torch.mean(window)\n",
    "\n",
    "        # Update progress bar\n",
    "        i = i + 1\n",
    "        bar.update(i, Loss = torch.mean(window[:i]))\n",
    "        \n",
    "        ## Update loss\n",
    "        L = L / batch_step;\n",
    "        L.backward()\n",
    "\n",
    "        if ( i % batch_step == 0 or i == n_data ):\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean loss on the test set\n",
    "def test():    \n",
    "    model.eval()\n",
    "    totalL = 0\n",
    "\n",
    "    for i, data in progressbar.progressbar(enumerate(test_loader)):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            L = loss(model(data.to(device)), data.y.to(device).squeeze())\n",
    "\n",
    "            totalL += L.item()\n",
    "\n",
    "    return totalL / 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, then test\n",
    "###### We train for 60 epochs, as in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "\n",
    "for epoch in range(60):\n",
    "    \n",
    "    print('Epoch {}'.format(epoch))\n",
    "    train()\n",
    "    \n",
    "    # Decay the learning rate for the last phase of training\n",
    "    if (epoch == 40):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.001\n",
    "\n",
    "\n",
    "testL = test();\n",
    "print('Mean test loss: {}'.format(testL), flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
